---
cidr_networks:
  container: 10.18.12.0/24
  tunnel: 10.20.12.0/24
  storage: 10.18.12.0/24

## Exclude IPs
used_ips:
  - "10.18.12.1,10.18.12.180"
  - "10.20.12.1,10.20.12.20"
  - "10.18.12.1,10.18.12.210"


global_overrides:
  internal_lb_vip_address: 10.18.12.200
  #
  # The below domain name must resolve to an IP address
  # in the CIDR specified in haproxy_keepalived_external_vip_cidr.
  # If using different protocols (https/http) for the public/internal
  # endpoints the two addresses must be different.
  #
  external_lb_vip_address: devtestopnstk.itgix.local
  tunnel_bridge: "br-vxlan"
  management_bridge: "br-mgmt"
  provider_networks:
    - network:
        container_bridge: "br-mgmt"
        container_type: "veth"
        container_interface: "eth1"
        ip_from_q: "container"
        type: "raw"
        group_binds:
          - all_containers
          - hosts
        is_container_address: true
        is_ssh_address: true
    - network:
        container_bridge: "br-vxlan"
        container_type: "veth"
        container_interface: "eth10"
        ip_from_q: "tunnel"
        type: "vxlan"
        range: "1:1000"
        net_name: "vxlan"
        group_binds:
          - neutron_linuxbridge_agent
    - network:
        container_bridge: "br-vlan"
        container_type: "veth"
        container_interface: "eth12"
        host_bind_override: "eth12"
        type: "flat"
        net_name: "flat"
        group_binds:
          - neutron_linuxbridge_agent
    - network:
        container_bridge: "br-vlan"
        container_type: "veth"
        container_interface: "eth11"
        type: "vlan"
        range: "1:1"
        net_name: "vlan"
        group_binds:
          - neutron_linuxbridge_agent
    - network:
        container_bridge: "br-storage"
        container_type: "veth"
        container_interface: "eth2"
        ip_from_q: "storage"
        type: "raw"
        group_binds:
          - glance_api
          - cinder_api
          - cinder_volume
          - nova_compute

###
### Infrastructure
###

# galera, memcache, rabbitmq, utility
shared-infra_hosts:
  infra1:
    ip: 10.18.12.10
  infra2:
    ip: 10.18.12.11
#  infra3:
#    ip: 172.29.236.13

# repository (apt cache, python packages, etc)
repo-infra_hosts:
  infra1:
    ip: 10.18.12.10
  infra2:
    ip: 10.18.12.11
#  infra3:
#    ip: 172.29.236.13

# load balancer
# Ideally the load balancer should not use the Infrastructure hosts.
# Dedicated hardware is best for improved performance and security.
haproxy_hosts:
  infra1:
    ip: 10.18.12.10
  infra2:
    ip: 10.18.12.11


# rsyslog server
log_hosts:
  log1:
    ip: 10.18.12.13 ## the mds for now

###
### OpenStack
###

# keystone
identity_hosts:
  infra1:
    ip: 10.18.12.10
  infra2:
    ip: 10.18.12.11
#  infra3:
#    ip: 172.29.236.13

# cinder api services
storage-infra_hosts:
  infra1:
    ip: 10.18.12.10
  infra2:
    ip: 10.18.12.11
#  infra3:
#    ip: 172.29.236.13

# glance
# The settings here are repeated for each infra host.
# They could instead be applied as global settings in
# user_variables, but are left here to illustrate that
# each container could have different storage targets.
image_hosts:
  infra1:
    ip: 10.18.12.10
  infra2:
    ip: 10.18.12.11


# nova api, conductor, etc services
compute-infra_hosts:
  infra1:
    ip: 10.18.12.10
  infra2:
    ip: 10.18.12.11

# heat
orchestration_hosts:
  infra1:
    ip: 10.18.12.10
  infra2:
    ip: 10.18.12.11

# horizon
dashboard_hosts:
  infra1:
    ip: 10.18.12.10
  infra2:
    ip: 10.18.12.11

# neutron server, agents (L3, etc)
network_hosts:
  infra1:
    ip: 10.18.12.10
  infra2:
    ip: 10.18.12.11

# ceilometer (telemetry data collection)
metering-infra_hosts:
  infra1:
    ip: 10.18.12.10
  infra2:
    ip: 10.18.12.11

# aodh (telemetry alarm service)
metering-alarm_hosts:
  infra1:
    ip: 10.18.12.10
  infra2:
    ip: 10.18.12.11

# gnocchi (telemetry metrics storage)
metrics_hosts:
  infra1:
    ip: 10.18.12.10
  infra2:
    ip: 10.18.12.11

# nova hypervisors
# Misho: commenting as we want to use the infra nodes as compute similar to AIO
#compute_hosts:
#  compute1:
#    ip: 10.18.12.16
#  compute2:
#a    ip: 10.18.12.17

# ceilometer compute agent (telemetry data collection)
#metering-compute_hosts:
#  compute1:
#    ip: 10.18.12.16
#  compute2:
#    ip: 10.18.12.17

# cinder volume hosts (NFS-backed)
# The settings here are repeated for each infra host.
# They could instead be applied as global settings in
# user_variables, but are left here to illustrate that
# each container could have different storage targets.
storage_hosts:
  infra1:
    ip: 10.18.12.10
    container_vars:
       cinder_storage_availability_zone: cinderAZ_1
       cinder_default_availability_zone: cinderAZ_1
       cinder_backends:
         limit_container_types: cinder_volume
         volumes_hdd:
           volume_driver: cinder.volume.drivers.rbd.RBDDriver
           rbd_pool: volumes
           rbd_ceph_conf: /etc/ceph/ceph.conf
           rbd_flatten_volume_from_snapshot: 'false'
           rbd_max_clone_depth: 5
           rbd_store_chunk_size: 4
           rados_connect_timeout: -1
           volume_backend_name: volumes_hdd
           rbd_user: "{{ cinder_ceph_client }}"
           rbd_secret_uuid: "{{ cinder_ceph_client_uuid }}"
  infra2:
    ip: 10.18.12.11
    container_vars:
       cinder_storage_availability_zone: cinderAZ_3
       cinder_default_availability_zone: cinderAZ_1
       cinder_backends:
         limit_container_types: cinder_volume
         volumes_hdd:
           volume_driver: cinder.volume.drivers.rbd.RBDDriver
           rbd_pool: volumes
           rbd_ceph_conf: /etc/ceph/ceph.conf
           rbd_flatten_volume_from_snapshot: 'false'
           rbd_max_clone_depth: 5
           rbd_store_chunk_size: 4
           rados_connect_timeout: -1
           volume_backend_name: volumes_hdd
           rbd_user: "{{ cinder_ceph_client }}"
           rbd_secret_uuid: "{{ cinder_ceph_client_uuid }}"